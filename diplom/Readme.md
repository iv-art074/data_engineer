### Дипломный проект по профессии Инженер данных ###  

#### I. Задание и рекомендации по выполнению работы. ####  
1. Необходимо разработать и задокументировать ETL-процессы заливки данных в хранилище, состоящее из слоёв:  
NDS - нормализованное хранилище и DDS - схема звезда;
Data Quality - опционально, будет большим преимуществом в вашей работе;  

на основании DDS построить в Tableau дашборды  
Рекомендации при выполнении работы:  
- ETL процессы можно делать с помощью Python (pandas) + SQL;  
- Оркестровку сделать с помощью Airflow;  

-опционально можно сделать отдельный слой метаданных в хранилище, а также дашборды на основании данных из этого слоя, где будет отображаться кол-во прогрузок и их статусы;  

Результат должен содержать: 
- дашборды  
- задокументированная схема хранилища данных  
- документированная схема ETL-процессов  

#### II. Данные ####  

Таблица продаж согласно заданию была взята с kaggle.com  
Новые данные из API получать не стал, т.к. нашел только один удобный источник - mockaroo, который использовался уже много раз.  
Воспользовался простой генерацией csv c сайта https://extendsclass.com/csv-generator.html#container-api  
Получил данные за другие кварталы.  
  
#### II. Схема хранилища данных ####  

#### NDS ####  
![airflow - nds_airflow](https://github.com/user-attachments/assets/b76576a1-6ccb-43ea-9cec-495e9eadaaeb)  

[Описание схемы](https://github.com/iv-art074/data_engineer/edit/main/diplom/data/nds_structure.md)  

#### DDS ####  
![airflow - dds_airflow](https://github.com/user-attachments/assets/8edc89a0-065d-4ff3-84b2-317d8f6d870d)  

[Описание схемы](https://github.com/iv-art074/data_engineer/blob/main/diplom/data/dds.md)  

#### III. Метаданные ####  

Схема Data Quality, примененная в DAG этой работы, состоит из нескольких элементов:  
- Логгирование и мониторинг: В каждом DAG фиксируется начало и завершение загрузки, статус выполнения (например, in_progress, completed, failed), количество обработанных строк и возможные сообщения об ошибках.
- Проверка и очистка данных: В get_data_dag выполняются очистка данных от пустых и дублированных строк, переименование столбцов для соответствия стандартам. В load_data_dag данные извлекаются с проверкой на дубли и корректность, а также вставляются с проверкой уникальности. Эти шаги обеспечивают целостность и надежность данных на каждом этапе обработки.  
- Обработка ошибок и управление исключениями: В случае ошибок в любом из DAG, статус обновляется на failed, записывается сообщение об ошибке, и выбрасывается  исключение. Включение повторных попыток выполнения задач в настройках DAG увеличивает вероятность успешного завершения процессов, минимизируя влияние временных проблем.  
- в структуре базы данных DDS введены специальные таблицы load_history и load_status для хранения статуса загрузок данных.  


