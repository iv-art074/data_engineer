### Дипломный проект по профессии Инженер данных ###  

#### I. Задание и рекомендации по выполнению работы. ####  
1. Необходимо разработать и задокументировать ETL-процессы заливки данных в хранилище, состоящее из слоёв:  
NDS - нормализованное хранилище и DDS - схема звезда;
Data Quality - опционально, будет большим преимуществом в вашей работе;  

на основании DDS построить в Tableau дашборды  
Рекомендации при выполнении работы:  
- ETL процессы можно делать с помощью Python (pandas) + SQL;  
- Оркестровку сделать с помощью Airflow;  

-опционально можно сделать отдельный слой метаданных в хранилище, а также дашборды на основании данных из этого слоя, где будет отображаться кол-во прогрузок и их статусы;  

Результат должен содержать: 
- дашборды  
- задокументированная схема хранилища данных  
- документированная схема ETL-процессов  

#### II. Данные ####  

Таблица продаж согласно заданию была взята с kaggle.com  
Новые данные из API получать не стал, т.к. нашел только один удобный источник - mockaroo, который использовался уже много раз.  
Воспользовался простой генерацией csv c сайта https://extendsclass.com/csv-generator.html#container-api  
Получил данные за другие кварталы.  
  
#### II. Схема хранилища данных ####  

#### NDS ####  
![airflow - nds_airflow](https://github.com/user-attachments/assets/b76576a1-6ccb-43ea-9cec-495e9eadaaeb)  

[Описание схемы](https://github.com/iv-art074/data_engineer/edit/main/diplom/data/nds_structure.md)  

#### DDS ####  
![airflow - dds_airflow](https://github.com/user-attachments/assets/8edc89a0-065d-4ff3-84b2-317d8f6d870d)  

[Описание схемы](https://github.com/iv-art074/data_engineer/blob/main/diplom/data/dds.md)  

#### III. Метаданные ####  

Схема Data Quality, примененная в DAG этой работы, состоит из нескольких элементов:  
- Логгирование и мониторинг: В каждом DAG фиксируется начало и завершение загрузки, статус выполнения (например, in_progress, completed, failed), количество обработанных строк и возможные сообщения об ошибках.
- Проверка и очистка данных: В get_data_dag выполняются очистка данных от пустых и дублированных строк, переименование столбцов для соответствия стандартам. В load_data_dag данные извлекаются с проверкой на дубли и корректность, а также вставляются с проверкой уникальности. Эти шаги обеспечивают целостность и надежность данных на каждом этапе обработки.  
- Обработка ошибок и управление исключениями: В случае ошибок в любом из DAG, статус обновляется на failed, записывается сообщение об ошибке, и выбрасывается  исключение. Включение повторных попыток выполнения задач в настройках DAG увеличивает вероятность успешного завершения процессов, минимизируя влияние временных проблем.  
- в структуре базы данных DDS введены специальные таблицы load_history и load_status для хранения статуса загрузок данных.  

#### IV. ETL-процессы ####  

![image](https://github.com/user-attachments/assets/33f798f0-674d-4c63-8a48-2a4f0ed5354a)  

ETL-процесс реализован на базе Airflow в Docker  

![image](https://github.com/user-attachments/assets/16f8e551-468a-4628-a86a-937b802ba803)  

Созданы три DAG:
- create_table_dag - генерация структуры SQL-таблиц NDS и DDS. Запускается по требованию  
- get_data_dag - производит чтение данных из CSV-файлов, попадающих в контейнер airflow-worker, очистку и подготовку этих данных, а затем их загрузке в таблицы базы данных. Очистка данных включает удаление пустых и дублированных строк, а также приведение форматов данных к необходимому виду. Подготовленные данные затем вставляются в соответствующие таблицы базы данных, обеспечивая их доступность для дальнейшего анализа и отчетности.  
В процессе обработки DAG записывает статус выполнения задач в таблицы базы данных, что позволяет отслеживать ход выполнения, успешные завершения и возможные ошибки. В случае возникновения ошибок.
- load_data_dag - переносит данные из промежуточной базы данных (NDS) в целевую базу данных (DDS). Этот процесс является завершающим этапом в цепочке задач ETL (Extract, Transform, Load) и играет ключевую роль в обеспечении актуальности и целостности данных в аналитической системе.  
Основная задача заключается в извлечении новых данных из базы данных NDS, их трансформации и загрузке в целевую базу данных DDS. DAG начинает с выполнения SQL-запроса к базе данных NDS для получения всех новых записей, которые еще не были перенесены по полю 'flashed'. Затем эти данные трансформируются и вставляются в соответствующие таблицы в базе данных DDS. Этот процесс включает проверку уникальности данных и использование команд ON CONFLICT для предотвращения дублирования записей.  
DAG также включает механизмы обновления статуса записей в базе данных NDS, отмечая их как перенесенные, что предотвращает повторный перенос тех же данных. Весь процесс логгируется.  

#### V. Dashboard ####  

Дашборд реализован в Tableau  
Создано подключение к SQL-серверу Airflow в Docker для возможности динамического отслеживания данных  

![image](https://github.com/user-attachments/assets/5213de1e-84ad-4676-b01c-0106f00b5758)  

Для примера созданы:  
- отчет по продажам филиалов по недельно  
- отчет по типу платежей по полу покупателя  
- продажи товаров в разрезе категорий  
- лог загрузок строк данных  
